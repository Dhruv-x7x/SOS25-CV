Details about this assignment can be found [on the course webpage](http://cs231n.github.io/), under Assignment #1 of Spring 2017.

## Assignment 1

### KNN 

```./cs231n/knn.ipynb``` contains an implementation of KNN on the CIFAR10 dataset. The training data included 50000 images and the testing data had 10000. I used a subset of this for the exercise, 5000 training and 500 testing. The highest accuracy for this subset was 28.8% with k = 10. I found the optimal k using k-fold cross-validation. 

On using the full dataset, I recorded the highest accuracy to be 37% after computing for 60 minutes. I can conclude that KNN is not good for images as computation takes too long during prediction time and the accuracy is very low even for a decently large training dataset. 

---

#### Numpy functions

- ```np.array_split(array, no_of_splits)```: using this function you can easily split your training data into the number of folds that you want.
- ```np.vstack()```: vertically stack arrays to form a matrix

```python
import numpy as np
# Example 1: Stacking 1D arrays
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])
c = np.array([7, 8, 9])

result = np.vstack([a, b, c])
print(result)
# Output:
# [[1 2 3]
#  [4 5 6]
#  [7 8 9]]
print(f"Shape: {result.shape}")  # Shape: (3, 3)
```
- ```np.hstack()```: horizontally stacks arrays to form a 1D array

```python
# Example 1: Stacking 1D arrays
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])
c = np.array([7, 8, 9])

result = np.hstack([a, b, c])
print(result)
# Output: [1 2 3 4 5 6 7 8 9]
print(f"Shape: {result.shape}")  # Shape: (9,)
```

---

#### Finding KNN in py

```python
closest_y = self.y_train[np.argsort(dists[i])[:k]]
y_pred[i] = Counter(closest_y).most_common(1)[0][0]
```

With just two lines we can find the k closest points to our input image. `dists` array contains the distances of each y_train to x_train. Each row is y_train[i] and every column contains differences of all y_train to x_train[j]. `np.argsort` sorts in increasing order, then we slice the first k smallest values. The second line calls on `Counter()` from `collections` which has a `.most_common()` attribute and using this we can find the majority class.

---

### Multi-Class Support Vector Machine

`./svm.ipynb` contains my work for this assignment as well as `./cs231n/classifiers/linear_classifier.py`. I used both simple for loops as well as numpy broadcasting to see the difference. Numpy broadcasting makes everything much faster. I consistently saw a 10 times faster matrix computations. 

The loss curve looks pretty smooth. A total of 1500 iterations were done using a learning rate of 1e-7 and regularization strength of 2.5e4. Training took 7.27 seconds. The training accuracy stood at 37.3% with validation accuracy at 38.8%. It shows that the model is not overfitting and is able to generalize to some degree, although the accuracy is pretty low. The reason is straightforward, this is a linear classifier, when we get to training non-linear classifiers, we will record accuracies of 90+%. 

![image](https://github.com/user-attachments/assets/85c7511f-1a3d-49e5-8b07-2463d404b3f2)

#### Cross-validation to Tune Learning Rate and Regularization Strength

The best parameters turned out to be (lr, lambda) = (2e-7, 2.5e4) with validation accuracy almost 40%. The testing accuracy sat slightly lower at 38%. 

#### Visualizing the Weights

![image](https://github.com/user-attachments/assets/9a42f3e8-f82b-412a-a152-8aef75afb8f4)

The weights look very similar to their classes, as if all the images of that class were merged on top of each other. It's not any clear image, for example, the car class weights looks like a blurred out picture of a car from the front. SVM is a linear model that's why we see a sort of template for the weights which are matched with each training example. This is a well trained model and the weights do not look like pure noise.

#### Functions, Tricks and Mechanisms learned

- `%load_ext autoreload; %autoreload 2`: If you have external modules you are pulling functions and classes from into your jupyter notebook then this is a great tool to use. Say that you made a change in the external module, like adding debug messages to some function and now you want to use that modified function in your jupyter notebook, you will have to restart the kernel to see those changes. With these two lines of code, your jupter notebook will constantly keep reloading the external modules so any change you make outside is immediately available in your notebook.
- Batching using random indices generated by `np.random.choice`. We generate batch_size number of random indices from 0 to number of training datapoints. Replacement is allowed because it makes it more random.

```python
indices = np.random.choice(num_train, batch_size, replace=True)
X_batch = X[indices]
y_batch = y[indices]
```

- `correct_scores = scores[np.arange(num_train), y_batch]`: I had used `np.arange()` like this before for one-hot encoding but this was a good reminder. To find the correct scores, you go through each example, check the score at the column given by `y_batch` and here the row is indexed by `np.arange(num_train)`. It will list out the numbers in `num_train` range and for each index the array will get that index from `y_batch` and find the correct score.
- Summing all the positive numbers in an array, `loss += np.sum(margin[margin>0])`. Numpy broadcasting is super useful.
- Overloading functions using sub-classes

---

### Softmax

`./softmax.ipynb` contains the work for this part as well as `./cs231n/classifiers/softmax.py`. I used both simple for loops as well as numpy broadcasting to see the difference. Numpy broadcasting makes everything much faster. I consistently saw a 10 times faster matrix computations. 

I got an accuracy of 36% with the softmax classifier over the validation set and a testing accuracy of 35%. The best parameters turned out to be (lr, lambda) = (5e-7, 2.5e4).

#### Visualizing the Weights

![image](https://github.com/user-attachments/assets/b74782f0-aff0-404c-817c-5de83c215bce)

The weights look very similar to the weights of Multi-Class SVM classifier.

---
